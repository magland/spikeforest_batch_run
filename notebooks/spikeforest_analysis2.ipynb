{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "spikeforest_analysis2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "blWcqWOIhzkc",
        "rvCvUJ1JiZX1",
        "1vvFYgXjmetF"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/magland/spikeforest_batch_run/blob/master/notebooks/spikeforest_analysis2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "xfQAJFPFgbzE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# SpikeForest analysis"
      ]
    },
    {
      "metadata": {
        "id": "Fcx4kQoVt374",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This notebook represents a complete spikeforest analysis of the bionet studies. You should execute the first few cells and then skip down to the section of interest below."
      ]
    },
    {
      "metadata": {
        "id": "vtsE5P3EgXYr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Only run this cell if you are running this on a hosted runtime that does not have these packages installed\n",
        "# %%capture is used to suppress the output... this should take up to a minute to complete\n",
        "%%capture\n",
        "!pip install spikeforest\n",
        "!pip install git+https://github.com/magland/spikeforest_batch_run"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P3-FpNWUgWln",
        "colab_type": "code",
        "outputId": "0a480f9d-d36b-4542-b013-11e4c3d432ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Import the python packages -- autoreload is used for development purposes\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import os\n",
        "os.environ['VDOMR_MODE']='COLAB'\n",
        "\n",
        "import spikeforest as sf\n",
        "from kbucket import client as kb\n",
        "import vdomr as vd\n",
        "import batcho"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vdomr: using colab because of VDOMR_MODE environment variable\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_JHpizoTul8M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Configure readonly access to kbucket -- use this if you only want to browse the results ---\n",
        "sf.kbucketConfigRemote(name='spikeforest1-readonly')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3pHRiMtggkaT",
        "colab_type": "code",
        "outputId": "c575bbd3-1912-4b82-cf23-34593d631f27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "cell_type": "code",
      "source": [
        "## Configure read/write access to kbucket -- use this if you are preparing the studies or the processing batches\n",
        "sf.kbucketConfigRemote(name='spikeforest1-readwrite',ask_password=True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter password: ··········\n",
            "Pairio user set to spikeforest. Test succeeded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "blWcqWOIhzkc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Prepare recordings"
      ]
    },
    {
      "metadata": {
        "id": "HtbjJUirgm0s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def read_text_file(path):\n",
        "  path2=kb.realizeFile(path)\n",
        "  if path2 is None:\n",
        "    raise Exception('Unable to realize file: '+path)\n",
        "  with open(path2,'r') as f:\n",
        "    return f.read()\n",
        "  \n",
        "def prepare_bionet_studies(*,basedir, channels, study_set_name='bionet', suffix=''):\n",
        "  studies=[]\n",
        "  recordings=[]\n",
        "  names=['bionet_drift','bionet_shuffle','bionet_static']\n",
        "  for name in names:\n",
        "    study_name=name+suffix\n",
        "    study_dir=basedir+'/bionet/'+name\n",
        "    description=read_text_file(study_dir+'/readme.txt')\n",
        "    study0=dict(\n",
        "        name=study_name,\n",
        "        study_set=study_set_name,\n",
        "        directory=study_dir,\n",
        "        description=description\n",
        "    )\n",
        "    studies.append(study0)\n",
        "    dd=kb.readDir(study_dir)\n",
        "    for dsname in dd['dirs']:\n",
        "        dsdir='{}/{}'.format(study_dir,dsname)\n",
        "        rec0=dict(\n",
        "            name=dsname,\n",
        "            study=study_name,\n",
        "            description='',\n",
        "            directory=dsdir,\n",
        "            channels=channels\n",
        "        )\n",
        "        if len(rec0['channels'])>0:\n",
        "          units=sf.sf_batch.select_units_on_channels(\n",
        "              recording_dir=dsdir,\n",
        "              firings=dsdir+'/firings_true.mda',\n",
        "              channels=rec0['channels']\n",
        "          )\n",
        "          rec0['units_true']=units\n",
        "        recordings.append(rec0)\n",
        "  return studies, recordings\n",
        "\n",
        "def prepare_bionet8c_studies(*,basedir):\n",
        "  channels = list(range(8))\n",
        "  \n",
        "  studies, recordings = prepare_bionet_studies(basedir=basedir, channels=channels, study_set_name='bionet', suffix='_8c')\n",
        "  return studies, recordings\n",
        "\n",
        "\n",
        "def prepare_bionet32c_studies(*,basedir):\n",
        "  channels = list(range(32))\n",
        "  \n",
        "  studies, recordings = prepare_bionet_studies(basedir=basedir, channels=channels, study_set_name='bionet', suffix='_32c')\n",
        "  return studies, recordings\n",
        "\n",
        "\n",
        "def prepare_magland_synth_studies(*,basedir):\n",
        "  study_set_name='magland_synth'\n",
        "  studies=[]\n",
        "  recordings=[]\n",
        "  names=[]\n",
        "  names=names+['datasets_noise10_K10_C4','datasets_noise10_K10_C8']\n",
        "  names=names+['datasets_noise10_K20_C4','datasets_noise10_K20_C8']\n",
        "  names=names+['datasets_noise20_K10_C4','datasets_noise20_K10_C8']\n",
        "  names=names+['datasets_noise20_K20_C4','datasets_noise20_K20_C8']\n",
        "  description=read_text_file(basedir+'/magland_synth/readme.txt')\n",
        "  for name in names:\n",
        "    study_name='magland_synth_'+name[9:]\n",
        "    study_dir=basedir+'/magland_synth/'+name\n",
        "    study0=dict(\n",
        "        name=study_name,\n",
        "        study_set=study_set_name,\n",
        "        directory=study_dir,\n",
        "        description=description\n",
        "    )\n",
        "    studies.append(study0)\n",
        "    dd=kb.readDir(study_dir)\n",
        "    for dsname in dd['dirs']:\n",
        "        dsdir='{}/{}'.format(study_dir,dsname)\n",
        "        recordings.append(dict(\n",
        "            name=dsname,\n",
        "            study=study_name,\n",
        "            directory=dsdir,\n",
        "            description='One of the recordings in the {} study'.format(study_name)\n",
        "        ))\n",
        "  return studies, recordings\n",
        "\n",
        "def prepare_mearec_tetrode_studies(*,basedir):\n",
        "  study_set_name='mearec_tetrode'\n",
        "  studies=[]\n",
        "  recordings=[]\n",
        "  names=[]\n",
        "  names=names+['datasets_noise10_K10_C4','datasets_noise10_K20_C4']\n",
        "  names=names+['datasets_noise20_K10_C4','datasets_noise20_K20_C4']\n",
        "  description=read_text_file(basedir+'/mearec_synth/tetrode/readme.txt')\n",
        "  for name in names:\n",
        "    study_name='mearec_tetrode_'+name[9:]\n",
        "    study_dir=basedir+'/mearec_synth/tetrode/'+name\n",
        "    study0=dict(\n",
        "        name=study_name,\n",
        "        study_set=study_set_name,\n",
        "        directory=study_dir,\n",
        "        description=description\n",
        "    )\n",
        "    studies.append(study0)\n",
        "    dd=kb.readDir(study_dir)\n",
        "    for dsname in dd['dirs']:\n",
        "        dsdir='{}/{}'.format(study_dir,dsname)\n",
        "        recordings.append(dict(\n",
        "            name=dsname,\n",
        "            study=study_name,\n",
        "            directory=dsdir,\n",
        "            description='One of the recordings in the {} study'.format(study_name)\n",
        "        ))\n",
        "  return studies, recordings\n",
        "\n",
        "def prepare_mearec_neuronexus_studies(*,basedir):\n",
        "  study_set_name='mearec_neuronexus'\n",
        "  studies=[]\n",
        "  recordings=[]\n",
        "  names=[]\n",
        "  names=names+['datasets_noise10_K10_C32','datasets_noise10_K20_C32','datasets_noise10_K40_C32']\n",
        "  names=names+['datasets_noise20_K10_C32','datasets_noise20_K20_C32','datasets_noise20_K40_C32']\n",
        "  description=read_text_file(basedir+'/mearec_synth/neuronexus/readme.txt')\n",
        "  for name in names:\n",
        "    study_name='mearec_neuronexus_'+name[9:]\n",
        "    study_dir=basedir+'/mearec_synth/neuronexus/'+name\n",
        "    study0=dict(\n",
        "        name=study_name,\n",
        "        study_set=study_set_name,\n",
        "        directory=study_dir,\n",
        "        description=description\n",
        "    )\n",
        "    studies.append(study0)\n",
        "    dd=kb.readDir(study_dir)\n",
        "    for dsname in dd['dirs']:\n",
        "        dsdir='{}/{}'.format(study_dir,dsname)\n",
        "        recordings.append(dict(\n",
        "            name=dsname,\n",
        "            study=study_name,\n",
        "            directory=dsdir,\n",
        "            description='One of the recordings in the {} study'.format(study_name)\n",
        "        ))\n",
        "  return studies, recordings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BpXkEoS8gaMn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "basedir='kbucket://15734439d8cf/groundtruth'\n",
        "#basedir='/mnt/ceph/users/jjun/groundtruth'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b92joharg0G5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "studies,recordings=prepare_bionet8c_studies(basedir=basedir)\n",
        "kb.saveObject(dict(studies=studies,recordings=recordings),key=dict(name='spikeforest_bionet8c_recordings'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cB6hVj_J2VhG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "studies,recordings=prepare_bionet32c_studies(basedir=basedir)\n",
        "kb.saveObject(dict(studies=studies,recordings=recordings),key=dict(name='spikeforest_bionet32c_recordings'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z3tb8UfC3VDU",
        "colab_type": "code",
        "outputId": "402c79de-43a9-41e8-d71d-48fc8513cfa7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "cell_type": "code",
      "source": [
        "studies,recordings=prepare_mearec_neuronexus_studies(basedir=basedir)\n",
        "kb.saveObject(dict(studies=studies,recordings=recordings),key=dict(name='spikeforest_mearec_neuronexus_recordings'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading file --- (0.0 MB): https://kbucket.flatironinstitute.org/15734439d8cf/download/groundtruth/mearec_synth/neuronexus/readme.txt -> /home/magland/kbucket_cache/4/ea/4ea6e0b1cb2e41b9d83d73856fd3c93b9620cd18\n",
            "Already on server.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XCUMPH8JgXK7",
        "colab_type": "code",
        "outputId": "be792b39-fc1c-4ff2-c899-c43e61fd174a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "studies,recordings=prepare_magland_synth_studies(basedir=basedir)\n",
        "kb.saveObject(dict(studies=studies,recordings=recordings),key=dict(name='spikeforest_magland_synth_recordings'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Already on server.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rvCvUJ1JiZX1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Create summarize recordings batches"
      ]
    },
    {
      "metadata": {
        "id": "MWIJU5Gxic_f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_summarize_recordings_batch(*,recordings_name,batch_name):\n",
        "  print('Creating summarize_recordings batch: '+batch_name)\n",
        "  SF=sf.SFData()\n",
        "  SF.loadRecordings(key=dict(name=recordings_name))\n",
        "  \n",
        "  jobs=[]\n",
        "  for name in SF.studyNames():\n",
        "    study=SF.study(name)\n",
        "    for recname in study.recordingNames():\n",
        "      R=study.recording(recname)\n",
        "      job=dict(\n",
        "          command='summarize_recording',\n",
        "          label='summarize '+study.name()+'/'+R.name(),\n",
        "          recording=R.getObject()\n",
        "      )\n",
        "      jobs.append(job)\n",
        "  batch=dict(jobs=jobs)\n",
        "  print('Number of jobs: {}'.format(len(jobs)))\n",
        "  batcho.set_batch(batch_name=batch_name,jobs=jobs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Vrw88QA5hACU",
        "colab_type": "code",
        "outputId": "f174cf98-c78b-4984-8a05-c13273c693bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "cell_type": "code",
      "source": [
        "create_summarize_recordings_batch(recordings_name='spikeforest_bionet8c_recordings',batch_name='summarize_recordings_bionet8c')\n",
        "create_summarize_recordings_batch(recordings_name='spikeforest_bionet32c_recordings',batch_name='summarize_recordings_bionet32c')\n",
        "create_summarize_recordings_batch(recordings_name='spikeforest_magland_synth_recordings',batch_name='summarize_recordings_magland_synth')\n",
        "create_summarize_recordings_batch(recordings_name='spikeforest_mearec_tetrode_recordings',batch_name='summarize_recordings_mearec_tetrode')\n",
        "create_summarize_recordings_batch(recordings_name='spikeforest_mearec_neuronexus_recordings',batch_name='summarize_recordings_mearec_neuronexus')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating summarize_recordings batch: summarize_recordings_bionet8c\n",
            "Loading recordings: {\"name\": \"spikeforest_bionet8c_recordings\"}\n",
            "Number of jobs: 36\n",
            "Creating summarize_recordings batch: summarize_recordings_bionet32c\n",
            "Loading recordings: {\"name\": \"spikeforest_bionet32c_recordings\"}\n",
            "Number of jobs: 36\n",
            "Creating summarize_recordings batch: summarize_recordings_magland_synth\n",
            "Loading recordings: {\"name\": \"spikeforest_magland_synth_recordings\"}\n",
            "Number of jobs: 80\n",
            "Creating summarize_recordings batch: summarize_recordings_mearec_tetrode\n",
            "Loading recordings: {\"name\": \"spikeforest_mearec_tetrode_recordings\"}\n",
            "Number of jobs: 40\n",
            "Creating summarize_recordings batch: summarize_recordings_mearec_neuronexus\n",
            "Loading recordings: {\"name\": \"spikeforest_mearec_neuronexus_recordings\"}\n",
            "Number of jobs: 60\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1kIpW9vPEjxg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Check the status of a batch\n",
        "statuses=batcho.get_batch_job_statuses(batch_name='summarize_recordings_bionet8c')\n",
        "for status in statuses:\n",
        "  print(status['job']['label'],status['status'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "weikHp2gjbwJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To run these batches, go to a computer with resources somewhere and run something like:\n",
        "\n",
        "```\n",
        "bin/sf_run_batch2 [name_of_batch] --run_prefix \"srun -c 2 -n 40\"\n",
        "```\n",
        "\n",
        "\"srun\" in Flatiron cluster reqiures you to run the following before\n",
        "```\n",
        "module load srun\n",
        "module load matlab\n",
        "```\n",
        "To use GPU, run\n",
        "```\n",
        "bin/sf_run_batch2 [name_of_batch] --run_prefix \"srun -c 2 -n 40 --gres=gpu:2 -p gpu\"\n",
        "```\n",
        "  \n",
        "where bin/sf_run_batch2 is found in the spikeforest2 repository.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "1vvFYgXjmetF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Create spike sorting batches"
      ]
    },
    {
      "metadata": {
        "id": "7toleMsBnMy6",
        "colab_type": "code",
        "outputId": "01693ed5-e8ef-411d-d68a-8528c2e4a0f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        }
      },
      "cell_type": "code",
      "source": [
        "SF=sf.SFData()\n",
        "SF.loadRecordings(key=dict(name='spikeforest_bionet8c_recordings'))\n",
        "SF.loadRecordings(key=dict(name='spikeforest_bionet32c_recordings'))\n",
        "SF.loadRecordings(key=dict(name='spikeforest_magland_synth_recordings'))\n",
        "SF.loadRecordings(key=dict(name='spikeforest_mearec_tetrode_recordings'))\n",
        "SF.loadRecordings(key=dict(name='spikeforest_mearec_neuronexus_recordings'))\n",
        "SF.loadProcessingBatch(batch_name='summarize_recordings_bionet8c')\n",
        "SF.loadProcessingBatch(batch_name='summarize_recordings_bionet32c')\n",
        "SF.loadProcessingBatch(batch_name='summarize_recordings_magland_synth')\n",
        "SF.loadProcessingBatch(batch_name='summarize_recordings_mearec_tetrode')\n",
        "SF.loadProcessingBatch(batch_name='summarize_recordings_mearec_neuronexus')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading recordings: {\"name\": \"spikeforest_bionet8c_recordings\"}\n",
            "Loading recordings: {\"name\": \"spikeforest_bionet32c_recordings\"}\n",
            "Loading recordings: {\"name\": \"spikeforest_magland_synth_recordings\"}\n",
            "Loading recordings: {\"name\": \"spikeforest_mearec_tetrode_recordings\"}\n",
            "Loading recordings: {\"name\": \"spikeforest_mearec_neuronexus_recordings\"}\n",
            "Loading processing batch: {\"name\": \"batcho_batch_results\", \"batch_name\": \"summarize_recordings_bionet8c\"}\n",
            "Loaded 0 sorting results and 36 recording summary results\n",
            "Loading processing batch: {\"name\": \"batcho_batch_results\", \"batch_name\": \"summarize_recordings_bionet32c\"}\n",
            "Loaded 0 sorting results and 36 recording summary results\n",
            "Loading processing batch: {\"name\": \"batcho_batch_results\", \"batch_name\": \"summarize_recordings_magland_synth\"}\n",
            "Downloading file --- (0.0 MB): http://132.249.245.245:24351/7317cea8265b/download/9/41/94130ea9ec2ef54550fee6ee3c8c73baeaddaeea -> /home/magland/kbucket_cache/9/41/94130ea9ec2ef54550fee6ee3c8c73baeaddaeea\n",
            "Loaded 0 sorting results and 80 recording summary results\n",
            "Loading processing batch: {\"name\": \"batcho_batch_results\", \"batch_name\": \"summarize_recordings_mearec_tetrode\"}\n",
            "Downloading file --- (0.0 MB): http://132.249.245.245:24351/7317cea8265b/download/7/e3/7e3de0efc16c0dba29a39198ba7b679442b3bb9a -> /home/magland/kbucket_cache/7/e3/7e3de0efc16c0dba29a39198ba7b679442b3bb9a\n",
            "Loaded 0 sorting results and 40 recording summary results\n",
            "Loading processing batch: {\"name\": \"batcho_batch_results\", \"batch_name\": \"summarize_recordings_mearec_neuronexus\"}\n",
            "Downloading file --- (0.0 MB): http://132.249.245.245:24351/7317cea8265b/download/4/b3/4b3d79a22131f693baaf140a9ccba2aff0c8a304 -> /home/magland/kbucket_cache/4/b3/4b3d79a22131f693baaf140a9ccba2aff0c8a304\n",
            "Loaded 0 sorting results and 60 recording summary results\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2PHEdeuBnVJl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sorter_ms4_thr3=dict(\n",
        "    name='MountainSort4-thr3',\n",
        "    processor_name='MountainSort4',\n",
        "    params=dict(\n",
        "        detect_sign=-1,\n",
        "        adjacency_radius=50,\n",
        "        detect_threshold=3\n",
        "    )\n",
        ")\n",
        "\n",
        "sorter_irc_tetrode=dict(\n",
        "    name='IronClust-tetrode',\n",
        "    processor_name='IronClust',\n",
        "    params=dict(\n",
        "        detect_sign=-1,\n",
        "        adjacency_radius=50,\n",
        "        detect_threshold=5,\n",
        "        prm_template_name=\"tetrode_template.prm\"\n",
        "    )\n",
        ")\n",
        "\n",
        "sorter_irc_drift=dict(\n",
        "    name='IronClust-drift',\n",
        "    processor_name='IronClust',\n",
        "    params=dict(\n",
        "        detect_sign=-1,\n",
        "        adjacency_radius=50,\n",
        "        prm_template_name=\"template_drift.prm\"\n",
        "    )\n",
        ")\n",
        "\n",
        "sorter_sc=dict(\n",
        "    name='SpykingCircus',\n",
        "    processor_name='SpykingCircus',\n",
        "    params=dict(\n",
        "        detect_sign=-1,\n",
        "        adjacency_radius=50\n",
        "    )\n",
        ")\n",
        "\n",
        "sorter_ks=dict(\n",
        "    name='KiloSort',\n",
        "    processor_name='KiloSort',\n",
        "    params=dict(\n",
        "        detect_sign=-1,\n",
        "        adjacency_radius=50\n",
        "    )\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NhvWMXK7mSYu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_sorting_batch(*,recordings_name,batch_name,sorters):\n",
        "  print('Creating sorting batch: '+batch_name)\n",
        "  SF=sf.SFData()\n",
        "  SF.loadRecordings(key=dict(name=recordings_name))\n",
        "  \n",
        "  jobs=[]\n",
        "  for name in SF.studyNames():\n",
        "    study=SF.study(name)\n",
        "    for rname in study.recordingNames():\n",
        "      R=study.recording(rname)\n",
        "      for sorter in sorters:\n",
        "        job=dict(\n",
        "          command='sort_recording',\n",
        "          label=sorter['name']+': '+R.name(),\n",
        "          recording=R.getObject(),\n",
        "          sorter=sorter\n",
        "        )\n",
        "        jobs.append(job)\n",
        "\n",
        "  print('Number of jobs: {}'.format(len(jobs)))\n",
        "  batcho.set_batch(batch_name=batch_name,jobs=jobs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jartwMT-_q_z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# create batches\n",
        "\n",
        "vs_sorters = ['ms4', 'irc', 'sc', 'ks']\n",
        "v_sorters_tetrode = [sorter_ms4_thr3, sorter_irc_tetrode, sorter_sc, sorter_ks]\n",
        "v_sorters_siprobe = [sorter_ms4_thr3, sorter_irc_drift, sorter_sc, sorter_ks]\n",
        "vs_recordings_tetrode = ['magland_synth', 'mearec_tetrode']  \n",
        "vs_recordings_siprobe = ['bionet8c', 'bionet32c', 'mearec_neuronexus']\n",
        "vs_recordings = vs_recordings_tetrode + vs_recordings_siprobe\n",
        "\n",
        "\n",
        "for recording in vs_recordings_tetrode:\n",
        "  for sorter, s_sorter in zip(v_sorters_tetrode, vs_sorters):\n",
        "    recordings_name = 'spikeforest_{}_recordings'.format(recording)\n",
        "    batch_name = '{}_{}'.format(s_sorter, recording)\n",
        "    #print(recordings_name, batch_name)\n",
        "    create_sorting_batch(recordings_name=recordings_name, batch_name=batch_name, sorters=[sorter])\n",
        "  print()\n",
        "    \n",
        "for recording in vs_recordings_siprobe:\n",
        "  for sorter, s_sorter in zip(v_sorters_siprobe, vs_sorters):\n",
        "    recordings_name = 'spikeforest_{}_recordings'.format(recording)\n",
        "    batch_name = '{}_{}'.format(s_sorter, recording)\n",
        "    #print(recordings_name, batch_name)    \n",
        "    create_sorting_batch(recordings_name=recordings_name, batch_name=batch_name, sorters=[sorter])\n",
        "  print()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W2VTozxFnoE3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "To run these sorting batches, follow the instructions above."
      ]
    }
  ]
}